version: "3.7"
services:  
    # postgres used by airflow
    postgres:
        image: postgres:9.6
        networks:
            - default_net
        #volumes: 
            # Create Test database on Postgresql
        #    - ./docker-airflow/pg-init-scripts:/docker-entrypoint-initdb.d
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow
        ports:
            - "5432:5432"
    
    zookeeper:
        image: confluentinc/cp-zookeeper:latest
        environment:
            ZOOKEEPER_CLIENT_PORT: 2181
            ZOOKEEPER_TICK_TIME: 2000
        ports:
            - 22181:2181
        networks:
            - default_net
  
    kafka:
        image: confluentinc/cp-kafka:latest
        depends_on:
            - zookeeper
        ports:
            - 29092:29092
        networks:
            - default_net
        environment:
            KAFKA_BROKER_ID: 1
            KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
            KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
            KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
            KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
            KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

    # airflow LocalExecutor
    airflow-webserver:
        # image: bitnami/airflow
        image: puckel/docker-airflow:latest
        restart: always
        networks:
            - default_net
        depends_on:
            - postgres
        environment:
            - LOAD_EX=n
            - EXECUTOR=Local
        volumes:
            - ../spark-airflow/dags:/usr/local/airflow/dags #DAG folder
            - ../spark-airflow//app:/usr/local/spark/app #Spark Scripts (Must be the same path in airflow and Spark Cluster)
            - ../spark-airflow/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
        ports:
            - "8282:8080"
        logging:
            options:
                max-size: 10m
                max-file: "3"
        # command: webserver
        healthcheck:
            test: ["CMD-SHELL", "[ -f /usr/local/airflow/airflow-webserver.pid ]"]
            interval: 30s
            timeout: 30s
            retries: 3

    # Spark with 2 workers
    spark:
        image: bitnami/spark:latest
        user: root
        hostname: spark
        networks:
            - default_net
        environment:
            - SPARK_MODE=master
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ../spark-airflow/app:/usr/local/spark/app # Spark scripts folder (Must be the same path in airflow and Spark Cluster)
            - ../spark-airflow/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)
        ports:
            - "8181:8080"
            - "7077:7077"

    spark-worker-1:
        image: bitnami/spark:latest
        user: root
        networks:
            - default_net
        environment:
            - SPARK_MODE=worker
            - SPARK_MASTER_URL=spark://spark:7077
            - SPARK_WORKER_MEMORY=1G
            - SPARK_WORKER_CORES=1
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ../spark-airflow/app:/usr/local/spark/app # Spark scripts folder (Must be the same path in airflow and Spark Cluster)
            - ../spark-airflow/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)

    spark-worker-2:
        image: bitnami/spark:latest
        user: root
        networks:
            - default_net
        environment:
            - SPARK_MODE=worker
            - SPARK_MASTER_URL=spark://spark:7077
            - SPARK_WORKER_MEMORY=1G
            - SPARK_WORKER_CORES=1
            - SPARK_RPC_AUTHENTICATION_ENABLED=no
            - SPARK_RPC_ENCRYPTION_ENABLED=no
            - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
            - SPARK_SSL_ENABLED=no
        volumes:
            - ../spark-airflow/app:/usr/local/spark/app # Spark scripts folder (Must be the same path in airflow and Spark Cluster)
            - ../spark-airflow/resources:/usr/local/spark/resources #Resources folder (Must be the same path in airflow and Spark Cluster)

networks:
    default_net:
